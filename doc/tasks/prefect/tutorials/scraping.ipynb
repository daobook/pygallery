{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从网站提取数据\n",
    "\n",
    "使用 Prefect 获取并分析大量数据\n",
    "\n",
    "现在，你将通过构建 GitHub 问题分析管道来学习如何处理数据依赖关系和摄取大量数据。\n",
    "\n",
    "现实世界中处理网络数据时可能会遇到额外的挑战：\n",
    "\n",
    "- API 请求可能会失败或返回缺失或格式错误的数据。\n",
    "- 你需要进行多次依赖的 API 调用。\n",
    "- 当你事先不知道有多少数据可用时，你需要摄取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "temp_dir = Path(\".temp\")\n",
    "# 创建临时目录\n",
    "if not temp_dir.exists():\n",
    "    temp_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置错误处理机制\n",
    "\n",
    "通过抛出和捕获错误来优雅地处理它们。例如，如果从API中没有收到2xx的响应码，就抛出一个异常并记录这个错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from prefect import task\n",
    "\n",
    "@task(log_prints=True)\n",
    "def fetch_page_of_issues(repo: str, page: int = 1) -> dict | None:\n",
    "    \"\"\"Fetch a page of issues for a GitHub repository\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\n",
    "            f\"https://api.github.com/repos/{repo}/issues\",\n",
    "            params={\"page\": page, \"state\": \"all\", \"per_page\": 100}\n",
    "        )\n",
    "        response.raise_for_status() # Raise an exception if the response is not a 2xx status code\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching issues for {repo}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .temp/error_handling.py\n"
     ]
    }
   ],
   "source": [
    "%%file {temp_dir}/error_handling.py\n",
    "from typing import List, Optional\n",
    "import httpx\n",
    "\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    for repo in repos:\n",
    "        print(f\"Analyzing {repo}...\")\n",
    "        \n",
    "        # Fetch and analyze all issues\n",
    "        fetch_page_of_issues(repo)\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def fetch_page_of_issues(repo: str, page: int = 1) -> Optional[dict]:\n",
    "    \"\"\"Fetch a page of issues for a GitHub repository\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\n",
    "            f\"https://api.github.com/repos/{repo}/issues\",\n",
    "            params={\"page\": page, \"state\": \"all\", \"per_page\": 100}\n",
    "        )\n",
    "        response.raise_for_status() # Raise an exception if the response is not a 2xx status code\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching issues for {repo}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_repo_health([\n",
    "        \"PrefectHQ/prefect\",\n",
    "        \"this-repo-does-not-exist/404\" # This repo will trigger an error\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摄取大量数据\n",
    "使用分页获取大量数据，并同时运行任务以高效分析数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from prefect import flow\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    all_issues = []\n",
    "\n",
    "    for repo in repos:\n",
    "        for page in range(1, 3):  # Get first 2 pages\n",
    "            issues = fetch_page_of_issues(repo, page)\n",
    "            if not issues:\n",
    "                break\n",
    "            all_issues.extend(issues)\n",
    "    \n",
    "    # Run issue analysis tasks concurrently\n",
    "    for issue in all_issues:\n",
    "        analyze_issue.submit(issue) # Submit each task to a task runner\n",
    "    \n",
    "    # Wait for all analysis tasks to complete\n",
    "    for detail in issue_details:\n",
    "        result = detail.result() # Block until the task has completed\n",
    "        print(f\"Analyzed issue #{result['number']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .temp/large_data.py\n"
     ]
    }
   ],
   "source": [
    "%%file {temp_dir}/large_data.py\n",
    "from typing import List, Optional\n",
    "import httpx\n",
    "\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    for repo in repos:\n",
    "        print(f\"Analyzing {repo}...\")\n",
    "        \n",
    "        # Fetch and analyze all issues\n",
    "        fetch_repo_issues(repo)\n",
    "\n",
    "\n",
    "@flow\n",
    "def fetch_repo_issues(repo: str):\n",
    "    \"\"\"Fetch all issues for a single repository\"\"\"\n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    \n",
    "    for page in range(1, 3):  # Limit to 2 pages to avoid hitting rate limits\n",
    "        issues = fetch_page_of_issues(repo, page)\n",
    "        if not issues or len(issues) == 0:\n",
    "            break\n",
    "        all_issues.extend(issues)\n",
    "        page += 1\n",
    "\n",
    "    issue_details = []\n",
    "    for issue in all_issues[:5]:  # Limit to 5 issues to avoid hitting rate limits\n",
    "        issue_details.append(\n",
    "            fetch_issue_details.submit(repo, issue['number'])  # Submit each task to a task runner\n",
    "        )\n",
    "    \n",
    "    details = []\n",
    "    for issue in issue_details:\n",
    "        details.append(issue.result())\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def fetch_page_of_issues(repo: str, page: int = 1) -> Optional[dict]:\n",
    "    \"\"\"Fetch a page of issues for a GitHub repository\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\n",
    "            f\"https://api.github.com/repos/{repo}/issues\",\n",
    "            params={\"page\": page, \"state\": \"all\", \"per_page\": 100}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching issues for {repo}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@task\n",
    "def fetch_issue_details(repo: str, issue_number: int) -> dict:\n",
    "    \"\"\"Fetch detailed information about a specific issue\"\"\"\n",
    "    response = httpx.get(f\"https://api.github.com/repos/{repo}/issues/{issue_number}\")\n",
    "    issue_data = response.json()\n",
    "    \n",
    "    return issue_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_repo_health([\n",
    "        \"PrefectHQ/prefect\",\n",
    "        \"pydantic/pydantic\",\n",
    "        \"huggingface/transformers\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 请根据依赖的嵌套流程和任务来结构化代码\n",
    "利用嵌套流程和任务帮助更有效地分配任务并辅助调试。\n",
    "\n",
    "- 对于涉及多个步骤的更复杂操作，使用嵌套流程。\n",
    "- 对于较简单、原子性的操作，使用任务。\n",
    "\n",
    "下面是如何使用嵌套流程和任务的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@flow\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    for repo in repos:\n",
    "        \n",
    "        # Fetch and analyze all issues\n",
    "        issues = fetch_repo_issues(repo)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        resolution_rate = calculate_resolution_rate(issues)\n",
    "        # ...\n",
    "\n",
    "\n",
    "@flow\n",
    "def fetch_repo_issues(repo: str):\n",
    "    \"\"\"Nested flow: Fetch all data for a single repository\"\"\"\n",
    "\n",
    "    # ...\n",
    "\n",
    "\n",
    "@task\n",
    "def calculate_resolution_rate(issues: List[dict]) -> float:\n",
    "    \"\"\"Task: Calculate the percentage of closed issues\"\"\"\n",
    "\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .temp/structure_code.py\n"
     ]
    }
   ],
   "source": [
    "%%file {temp_dir}/structure_code.py\n",
    "from typing import List, Optional\n",
    "import httpx\n",
    "\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    for repo in repos:\n",
    "        print(f\"Analyzing {repo}...\")\n",
    "        \n",
    "        # Fetch and analyze all issues\n",
    "        issues = fetch_repo_issues(repo)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        resolution_rate = calculate_resolution_rate(issues)\n",
    "        \n",
    "        print(f\"Resolution rate: {resolution_rate:.1f}%\")\n",
    "\n",
    "\n",
    "@flow\n",
    "def fetch_repo_issues(repo: str):\n",
    "    \"\"\"Fetch all issues for a single repository\"\"\"\n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    \n",
    "    for page in range(1, 3):  # Limit to 2 pages to avoid hitting rate limits\n",
    "        issues = fetch_page_of_issues(repo, page)\n",
    "        if not issues or len(issues) == 0:\n",
    "            break\n",
    "        all_issues.extend(issues)\n",
    "        page += 1\n",
    "\n",
    "    issue_details = []\n",
    "    for issue in all_issues[:5]:  # Limit to 5 issues to avoid hitting rate limits\n",
    "        issue_details.append(\n",
    "            fetch_issue_details.submit(repo, issue['number'])\n",
    "        )\n",
    "    \n",
    "    details = []\n",
    "    for issue in issue_details:\n",
    "        details.append(issue.result())\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def fetch_page_of_issues(repo: str, page: int = 1) -> Optional[dict]:\n",
    "    \"\"\"Fetch a page of issues for a GitHub repository\"\"\"\n",
    "    try:\n",
    "        response = httpx.get(\n",
    "            f\"https://api.github.com/repos/{repo}/issues\",\n",
    "            params={\"page\": page, \"state\": \"all\", \"per_page\": 100}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching issues for {repo}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@task\n",
    "def fetch_issue_details(repo: str, issue_number: int) -> dict:\n",
    "    \"\"\"Fetch detailed information about a specific issue\"\"\"\n",
    "    response = httpx.get(f\"https://api.github.com/repos/{repo}/issues/{issue_number}\")\n",
    "    issue_data = response.json()\n",
    "    \n",
    "    return issue_data\n",
    "\n",
    "\n",
    "@task\n",
    "def calculate_resolution_rate(issues: List[dict]) -> float:\n",
    "    \"\"\"Calculate the percentage of closed issues\"\"\"\n",
    "    if not issues:\n",
    "        return 0\n",
    "    closed = sum(1 for issue in issues if issue['state'] == 'closed')\n",
    "    return (closed / len(issues)) * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_repo_health([\n",
    "        \"PrefectHQ/prefect\",\n",
    "        \"pydantic/pydantic\",\n",
    "        \"huggingface/transformers\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 综合起来\n",
    "这里展示了完整的流程，它结合了所有这些组件。还将添加重试、缓存和速率限制，以使工作流程更加健壮。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .temp/repo_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%file {temp_dir}/repo_analysis.py\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from statistics import mean\n",
    "from typing import List, Optional\n",
    "import httpx\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.concurrency.sync import rate_limit\n",
    "\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def analyze_repo_health(repos: List[str]):\n",
    "    \"\"\"Analyze issue health metrics for GitHub repositories\"\"\"\n",
    "    for repo in repos:\n",
    "        print(f\"Analyzing {repo}...\")\n",
    "        \n",
    "        # Fetch and analyze all issues\n",
    "        issues = fetch_repo_issues(repo)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_response_time = calculate_response_times(issues)\n",
    "        resolution_rate = calculate_resolution_rate(issues)\n",
    "        \n",
    "        print(f\"Average response time: {avg_response_time:.1f} hours\")\n",
    "        print(f\"Resolution rate: {resolution_rate:.1f}%\")\n",
    "\n",
    "\n",
    "@flow\n",
    "def fetch_repo_issues(repo: str):\n",
    "    \"\"\"Fetch all issues for a single repository\"\"\"\n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    \n",
    "    for page in range(1, 3):  # Limit to 2 pages to avoid hitting rate limits\n",
    "        issues = fetch_page_of_issues(repo, page)\n",
    "        if not issues or len(issues) == 0:\n",
    "            break\n",
    "        all_issues.extend(issues)\n",
    "        page += 1\n",
    "\n",
    "    issue_details = []\n",
    "    for issue in all_issues[:5]:  # Limit to 5 issues to avoid hitting rate limits\n",
    "        issue_details.append(\n",
    "            fetch_issue_details.submit(repo, issue['number'])\n",
    "        )\n",
    "    \n",
    "    details = []\n",
    "    for issue in issue_details:\n",
    "        details.append(issue.result())\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "@task(log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))\n",
    "def fetch_page_of_issues(repo: str, page: int = 1) -> Optional[dict]:\n",
    "    \"\"\"Fetch a page of issues for a GitHub repository\"\"\"\n",
    "    rate_limit(\"github-api\")\n",
    "    try:\n",
    "        response = httpx.get(\n",
    "            f\"https://api.github.com/repos/{repo}/issues\",\n",
    "            params={\"page\": page, \"state\": \"all\", \"per_page\": 100}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching issues for {repo}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@task(retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))\n",
    "def fetch_issue_details(repo: str, issue_number: int) -> dict:\n",
    "    \"\"\"Fetch detailed information about a specific issue\"\"\"\n",
    "    rate_limit(\"github-api\")\n",
    "    response = httpx.get(f\"https://api.github.com/repos/{repo}/issues/{issue_number}\")\n",
    "    issue_data = response.json()\n",
    "    \n",
    "    # Fetch comments for the issue\n",
    "    comments = fetch_comments(issue_data['comments_url'])\n",
    "    issue_data['comments_data'] = comments\n",
    "    \n",
    "    return issue_data\n",
    "\n",
    "\n",
    "@task(log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))\n",
    "def fetch_comments(comments_url: str) -> List[dict]:\n",
    "    \"\"\"Fetch comments for an issue\"\"\"\n",
    "    rate_limit(\"github-api\")\n",
    "    try:\n",
    "        response = httpx.get(comments_url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "@task\n",
    "def calculate_response_times(issues: List[dict]) -> float:\n",
    "    \"\"\"Calculate average time to first response for issues\"\"\"\n",
    "    response_times = []\n",
    "    \n",
    "    for issue in issues:\n",
    "        comments_data = issue.get('comments_data', [])\n",
    "        if comments_data:  # If there are comments\n",
    "            created = datetime.fromisoformat(issue['created_at'].replace('Z', '+00:00'))\n",
    "            first_comment = datetime.fromisoformat(\n",
    "                comments_data[0]['created_at'].replace('Z', '+00:00')\n",
    "            )\n",
    "            response_time = (first_comment - created).total_seconds() / 3600\n",
    "            response_times.append(response_time)\n",
    "\n",
    "    return mean(response_times) if response_times else 0\n",
    "\n",
    "\n",
    "@task\n",
    "def calculate_resolution_rate(issues: List[dict]) -> float:\n",
    "    \"\"\"Calculate the percentage of closed issues\"\"\"\n",
    "    if not issues:\n",
    "        return 0\n",
    "    closed = sum(1 for issue in issues if issue['state'] == 'closed')\n",
    "    return (closed / len(issues)) * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_repo_health([\n",
    "        \"PrefectHQ/prefect\",\n",
    "        \"pydantic/pydantic\",\n",
    "        \"huggingface/transformers\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在运行此代码之前，请确保已设置GitHub API的速率限制。\n",
    "\n",
    "```bash\n",
    "# GitHub has a rate limit of 60 unauthenticated requests per hour (~0.016 requests per second)\n",
    "prefect gcl create github-api --limit 60 --slot-decay-per-second 0.016\n",
    "```\n",
    "\n",
    "运行分析：\n",
    "\n",
    "```bash\n",
    "python repo_analysis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出应该类似于这样：\n",
    "```bash\n",
    "10:59:13.933 | INFO    | prefect.engine - Created flow run 'robust-kangaroo' for flow 'analyze-repo-health'\n",
    "10:59:13.934 | INFO    | prefect.engine - View at http://127.0.0.1:4200/runs/flow-run/abdf7f46-6d59-4857-99cd-9e265cadc4a7\n",
    "10:59:13.954 | INFO    | Flow run 'robust-kangaroo' - Analyzing PrefectHQ/prefect...\n",
    "...\n",
    "10:59:27.631 | INFO    | Flow run 'robust-kangaroo' - Average response time: 0.4 hours\n",
    "10:59:27.631 | INFO    | Flow run 'robust-kangaroo' - Resolution rate: 40.0%\n",
    "10:59:27.632 | INFO    | Flow run 'robust-kangaroo' - Analyzing pydantic/pydantic...\n",
    "...\n",
    "10:59:40.990 | INFO    | Flow run 'robust-kangaroo' - Average response time: 0.0 hours\n",
    "10:59:40.991 | INFO    | Flow run 'robust-kangaroo' - Resolution rate: 0.0%\n",
    "10:59:40.991 | INFO    | Flow run 'robust-kangaroo' - Analyzing huggingface/transformers...\n",
    "...\n",
    "10:59:54.225 | INFO    | Flow run 'robust-kangaroo' - Average response time: 1.1 hours\n",
    "10:59:54.225 | INFO    | Flow run 'robust-kangaroo' - Resolution rate: 0.0%\n",
    "10:59:54.240 | INFO    | Flow run 'robust-kangaroo' - Finished in state Completed()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
